import os
import streamlit as st
import pandas as pd
import numpy as np
import json
from datetime import datetime
import re

# LangChain ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# ë””ë ‰í† ë¦¬ ì„¤ì •
BACKDATA_DIR = os.path.join(os.path.dirname(__file__), 'backdata')
CHROMA_DIR = os.path.join(os.path.dirname(__file__), 'chroma_db')
os.makedirs(BACKDATA_DIR, exist_ok=True)

# API í‚¤ ì„¤ì • - ê¸°ì¡´ ì•±ê³¼ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©
import openai
import httpx
import ssl
import urllib3

# ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
DEBUG_MODE = False

# SSL ê²€ì¦ ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

# API í‚¤ ì„¤ì •
openai.api_key = "sk-k7ZAoJmlclL75pjwHgEcFw"
openai.api_base = "https://genai-sharedservice-americas.pwcinternal.com/"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ” KAIST ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ ê²€ì¦ ë„êµ¬", layout="wide")

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
/* ì „ì²´ ì•± ìŠ¤íƒ€ì¼ë§ */
.stApp {
    background-color: white;
    font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, Roboto, 'Helvetica Neue', 'Segoe UI', 'Apple SD Gothic Neo', 'Noto Sans KR', sans-serif;
}

/* í—¤ë” ìŠ¤íƒ€ì¼ë§ */
.header-container {
    padding: 1rem 0;
    border-bottom: 1px solid #e5e7eb;
    margin-bottom: 2rem;
}

/* íƒ€ì´í‹€ ìŠ¤íƒ€ì¼ë§ */
.main-title {
    color: #0a1c3e;
    font-size: 2rem !important;
    font-weight: 700 !important;
    margin: 0;
    padding: 0;
    line-height: 1.2;
    letter-spacing: -0.025em;
    background: linear-gradient(90deg, #1e3a8a, #3b82f6);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}

.subtitle {
    color: #4b5563 !important;
    font-size: 1.1rem !important;
    margin-top: 0.25rem;
    font-weight: 400;
}

/* ì¹´ë“œ ìŠ¤íƒ€ì¼ë§ */
.card {
    background-color: white;
    border-radius: 8px;
    padding: 1.5rem;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    margin-bottom: 1rem;
    border: 1px solid #e5e7eb;
}

/* í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§ */
.dataframe {
    width: 100%;
    border-collapse: collapse;
}

.dataframe th {
    background-color: #f1f5f9;
    font-weight: 600;
    text-align: left;
    padding: 0.75rem 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.dataframe td {
    padding: 0.75rem 1rem;
    border-bottom: 1px solid #e2e8f0;
}

/* ìœ„ë°˜ í•­ëª© ê°•ì¡° */
.violation {
    background-color: #fee2e2;
    color: #b91c1c;
    font-weight: 500;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ë§ */
.stButton button {
    background-color: #2563eb;
    color: white;
    border: none;
    padding: 0.5rem 1rem;
    border-radius: 8px;
    font-weight: 600;
    transition: all 0.3s ease;
}

.stButton button:hover {
    background-color: #1e40af;
}

/* ìœ„ì ¯ ê°„ê²© ì¡°ì • */
.stSelectbox, .stDateInput {
    margin-bottom: 1rem;
}

/* ë¡œë”© ìŠ¤í”¼ë„ˆ */
.stSpinner {
    margin-top: 2rem;
    margin-bottom: 2rem;
}

/* ì•Œë¦¼ ë©”ì‹œì§€ */
.st-success {
    background-color: #ecfdf5;
    color: #065f46;
}

.st-error {
    background-color: #fef2f2;
    color: #b91c1c;
}

.st-info {
    background-color: #eff6ff;
    color: #1e40af;
}

/* í…Œì´ë¸” ë‚´ ìƒíƒœ í‘œì‹œ */
.status-ok {
    color: #047857;
    font-weight: 500;
}

.status-warning {
    color: #b45309;
    font-weight: 500;
}

.status-violation {
    color: #b91c1c;
    font-weight: 500;
}

/* ì„¸ë¶€ ì •ë³´ íŒ¨ë„ */
.detail-panel {
    background-color: #f8fafc;
    border-radius: 8px;
    padding: 1rem;
    margin-top: 1rem;
    border: 1px solid #e2e8f0;
}

/* ê·œì • ì¸ìš© ìŠ¤íƒ€ì¼ */
.regulation-citation {
    background-color: #f1f5f9;
    padding: 0.75rem;
    border-left: 4px solid #3b82f6;
    margin: 1rem 0;
    font-size: 0.9rem;
}
</style>
""", unsafe_allow_html=True)

# í—¤ë” ì»´í¬ë„ŒíŠ¸
st.markdown('<div class="header-container">', unsafe_allow_html=True)
st.markdown('<h1 class="main-title">KAIST ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ ê²€ì¦ ë„êµ¬</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">íšŒê³„ ê·œì •ì— ë”°ë¥¸ ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ í•­ëª© ê²€ì¦ ë° ìœ„ë°˜ ì‚¬í•­ ì‹ë³„</p>', unsafe_allow_html=True)
st.markdown('</div>', unsafe_allow_html=True)

# ë²¡í„°DB ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_vector_db():
    try:
        embeddings = OpenAIEmbeddings(
            openai_api_key=openai.api_key,
            openai_api_base=openai.api_base,
            model="azure.text-embedding-3-large"
        )
        db = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 3})
        
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            temperature=0,
            openai_api_key=openai.api_key,
            openai_api_base=openai.api_base,
            model_name="openai.gpt-4.1-mini-2025-04-14",
            request_timeout=60,
            http_client=httpx.Client(verify=False),
        )
        
        # ê²€ì¦ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        system_message = SystemMessagePromptTemplate.from_template(
            "ë„ˆëŠ” KAIST íšŒê³„ê·œì •ì— ë”°ë¼ ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ í•­ëª©ì„ ê²€ì¦í•˜ëŠ” ë„êµ¬ì•¼. "
            "ê° í•­ëª©ì´ ê·œì •ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ê²€í† í•˜ê³ , ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” í•­ëª©ì€ ê´€ë ¨ ê·œì •ê³¼ í•¨ê»˜ ëª…í™•íˆ í‘œì‹œí•´ì¤˜."
        )
        human_message = HumanMessagePromptTemplate.from_template(
            "ë‹¤ìŒ ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ í•­ëª©ì´ KAIST íšŒê³„ê·œì •ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ê²€í† í•´ì¤˜:\n\n"
            "í•­ëª© ì •ë³´: {item_info}\n\n"
            "ê·œì • ë§¥ë½: {context}\n\n"
            "ì´ í•­ëª©ì´ ê·œì •ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ì—¬ë¶€ì™€ ê·¸ ì´ìœ ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "```json\n"
            "{{\n"
            "  \"violation\": true/false,\n"
            "  \"violation_type\": \"ìœ„ë°˜ ìœ í˜• (ì˜ˆ: í•œë„ì´ˆê³¼, ë¯¸ìŠ¹ì¸ì§€ì¶œ, ì¦ë¹™ë¶€ì¡± ë“±)\",\n"
            "  \"explanation\": \"ìœ„ë°˜ ì´ìœ ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…\",\n"
            "  \"regulation_reference\": \"ê´€ë ¨ ê·œì • ë° ì¡°í•­ ë²ˆí˜¸\"\n"
            "}}\n"
            "```\n"
            "ê·œì •ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” violationì„ falseë¡œ ì„¤ì •í•˜ê³ , violation_typeì€ \"ì—†ìŒ\"ìœ¼ë¡œ ì„¤ì •í•´ì¤˜."
        )
        prompt = ChatPromptTemplate.from_messages([system_message, human_message])
        
        # QA ì²´ì¸ ìƒì„±
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
        
        return qa, retriever
    except Exception as e:
        st.error(f"ë²¡í„°DB ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

# ìë™ ì—´ ë§¤í•‘ í•¨ìˆ˜ ì¶”ê°€
def auto_map_columns(df_columns):
    # í•„ìˆ˜ í•„ë“œì™€ ê°€ëŠ¥í•œ ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ëª…ë“¤ì˜ ë§¤í•‘
    field_mappings = {
        # í•µì‹¬ ê¸ˆì•¡ ê´€ë ¨ í•„ë“œ
        'ê¸ˆì•¡': ['ê²°ì˜ê¸ˆì•¡', 'ì†Œê³„', 'ê³µê¸‰ê°€ì•¡'],
        'ë¶€ê°€ì„¸': ['ë¶€ê°€ê°€ì¹˜ì„¸'],
        'ë¯¸ì§€ê¸‰ê¸ˆ': ['ë¯¸ì§€ê¸‰ê¸ˆ'],
        
        # ê±°ë˜ ì •ë³´ í•„ë“œ
        'ì§€ì¶œì¼ì': ['ê±°ë˜ì¼ì', 'ì§€ê¸‰ì˜ˆì •ì¼'],
        'ì§€ì¶œë‚´ì—­': ['ë‚´ìš©', 'ê³„ì •ëª…'],
        'ì§€ì¶œì²˜': ['ì§€ê¸‰ê±°ë˜ì²˜', 'ê±°ë˜ì²˜'],
        'ì¦ë¹™ìœ í˜•': ['ì¦ë¹™ìœ í˜•'],
        
        # ê´€ë¦¬/ì¶”ì  í•„ë“œ
        'ë¬¸ì„œë²ˆí˜¸': ['ê²°ì˜ì„œë²ˆí˜¸', 'ë¬¸ì„œë²ˆí˜¸'],
        'ì •ì‚°ìƒíƒœ': ['ì •ì‚°ëŒ€ìƒìƒíƒœ', 'ê²°ì˜ìƒíƒœ'],
        'ë°˜ë‚©ì—¬ë¶€': ['ë°˜ë‚©ì—¬ë¶€'],
        
        # ë¶€ê°€ ì •ë³´ í•„ë“œ
        'ì†Œë“êµ¬ë¶„': ['ì†Œë“ìœ í˜•'],
        'ê³¼ì„¸ì—¬ë¶€': ['ê³¼ì„¸ì‚¬ì—…ì—¬ë¶€'],
        'ì§€ê¸‰ê·¸ë£¹': ['ì§€ê¸‰ê·¸ë£¹'],
        'ì²˜ë¦¬ë¶€ì„œ': ['ê¸°ì•ˆìë¶€ì„œ']
    }
    
    # ê²°ê³¼ ë§¤í•‘ ì €ì¥
    mapping_result = {}
    
    # ê° í•„ë“œì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°
    for field, possible_matches in field_mappings.items():
        # ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­ ì‹œë„
        for match in possible_matches:
            if match in df_columns:
                mapping_result[field] = match
                break
        
        # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ Noneìœ¼ë¡œ ì„¤ì •
        if field not in mapping_result:
            mapping_result[field] = None
    
    return mapping_result

# ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ìˆ˜ì •
def load_unpaid_data():
    files = []
    for file in os.listdir(BACKDATA_DIR):
        if file.endswith('.csv') or file.endswith('.xlsx') or file.endswith('.xls'):
            files.append(file)
    
    if not files:
        st.warning("backdata í´ë”ì— ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ í•´ë‹¹ í´ë”ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return None, None
    
    # íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    selected_file = st.selectbox("ë¶„ì„í•  íŒŒì¼ ì„ íƒ", files)
    file_path = os.path.join(BACKDATA_DIR, selected_file)
    
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path, encoding='utf-8', nrows=100)  # ìƒìœ„ 100ì¤„ë§Œ ì½ê¸°
        else:
            df = pd.read_excel(file_path, nrows=100)  # ìƒìœ„ 100ì¤„ë§Œ ì½ê¸°
        
        # ìë™ ì—´ ë§¤í•‘ ìˆ˜í–‰
        auto_mapping = auto_map_columns(df.columns)
        
        # ì „ì²´ ë°ì´í„° ë‹¤ì‹œ ì½ê¸°
        if file_path.endswith('.csv'):
            full_df = pd.read_csv(file_path, encoding='utf-8')
        else:
            full_df = pd.read_excel(file_path)
        
        return full_df, auto_mapping
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

# í•­ëª© ê²€ì¦ í•¨ìˆ˜
def validate_item(item, qa):
    try:
        # ê¸°ë³¸ ê²€ì¦ ê·œì¹™ ì ìš©
        validation_checks = []
        
        # 1. ê¸ˆì•¡ ê´€ë ¨ ê²€ì¦
        if 'ê¸ˆì•¡' in item and 'ë¶€ê°€ì„¸' in item:
            total = float(item['ê¸ˆì•¡'])
            tax = float(item.get('ë¶€ê°€ì„¸', 0))
            if tax > total * 0.1:  # ë¶€ê°€ì„¸ê°€ ê³µê¸‰ê°€ì•¡ì˜ 10%ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
                validation_checks.append("ë¶€ê°€ì„¸ ê¸ˆì•¡ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìŠµë‹ˆë‹¤.")
        
        # 2. ë¯¸ì§€ê¸‰ê¸ˆ ê²€ì¦
        if 'ë¯¸ì§€ê¸‰ê¸ˆ' in item and float(item['ë¯¸ì§€ê¸‰ê¸ˆ']) > 0:
            validation_checks.append("ë¯¸ì§€ê¸‰ê¸ˆì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        # 3. ì¦ë¹™ ê²€ì¦
        if 'ì¦ë¹™ìœ í˜•' in item and pd.isna(item['ì¦ë¹™ìœ í˜•']):
            validation_checks.append("ì¦ë¹™ì„œë¥˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 4. ë°˜ë‚© ì—¬ë¶€ ê²€ì¦
        if 'ë°˜ë‚©ì—¬ë¶€' in item and item['ë°˜ë‚©ì—¬ë¶€'] == 'Y':
            validation_checks.append("ë°˜ë‚© ì²˜ë¦¬ëœ í•­ëª©ì…ë‹ˆë‹¤.")
        
        # 5. ê³¼ì„¸ ì²˜ë¦¬ ê²€ì¦
        if 'ê³¼ì„¸ì—¬ë¶€' in item and 'ì†Œë“êµ¬ë¶„' in item:
            if item['ê³¼ì„¸ì—¬ë¶€'] == 'Y' and pd.isna(item['ì†Œë“êµ¬ë¶„']):
                validation_checks.append("ê³¼ì„¸ ëŒ€ìƒì´ë‚˜ ì†Œë“êµ¬ë¶„ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í•­ëª© ì •ë³´ ë¬¸ìì—´ êµ¬ì„±
        basic_info = []
        for key, value in item.items():
            if not pd.isna(value):  # nullì´ ì•„ë‹Œ ê°’ë§Œ í¬í•¨
                basic_info.append(f"{key}: {value}")
        
        item_info = "\n".join(basic_info)
        if validation_checks:
            item_info += "\n\nê¸°ë³¸ ê²€ì¦ ê²°ê³¼:\n" + "\n".join(f"- {check}" for check in validation_checks)
        
        # ê·œì • ê²€ìƒ‰ ë° ê²€ì¦
        result = qa({"query": f"ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ í•­ëª© ê²€ì¦: {item_info}"})
        answer = result["result"]
        
        # JSON ì‘ë‹µ ì¶”ì¶œ ë° ê¸°ë³¸ ê²€ì¦ ê²°ê³¼ í†µí•©
        json_match = re.search(r'```json\s*(.*?)\s*```', answer, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            response = json.loads(json_str)
            
            # ê¸°ë³¸ ê²€ì¦ ê²°ê³¼ í†µí•©
            if validation_checks:
                response["violation"] = True
                existing_explanation = response.get("explanation", "")
                checks_text = "\n- " + "\n- ".join(validation_checks)
                response["explanation"] = f"ê¸°ë³¸ ê²€ì¦ ê²°ê³¼:{checks_text}\n\nê·œì • ê²€ì¦ ê²°ê³¼:\n{existing_explanation}"
        else:
            # JSONì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
            response = {
                "violation": bool(validation_checks),
                "violation_type": "ê¸°ë³¸ ê²€ì¦" if validation_checks else "ë¶„ì„ ë¶ˆê°€",
                "explanation": "\n".join(validation_checks) if validation_checks else "ì‘ë‹µì—ì„œ JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "regulation_reference": "N/A"
            }
        
        return response
    except Exception as e:
        if DEBUG_MODE:
            st.error(f"í•­ëª© ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "violation": True,
            "violation_type": "ê²€ì¦ ì˜¤ë¥˜",
            "explanation": f"ê²€ì¦ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "regulation_reference": "N/A"
        }

# ê·œì • ìœ„ë°˜ ì˜ì‹¬ í•­ëª©ì— ëŒ€í•œ ëŒ€ì‹œë³´ë“œ í‘œì‹œ
def display_dashboard(df, results, summary):
    """ê²°ê³¼ ëŒ€ì‹œë³´ë“œ í‘œì‹œ - ìµœì í™” ë²„ì „"""
    # 1. ì „ì²´ ìš”ì•½
    st.subheader("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì „ì²´ ë°ì´í„°", f"{summary['ì „ì²´ ë°ì´í„° ìˆ˜']:,}ê±´")
    with col2:
        st.metric("ê¸°ë³¸ ê·œì¹™ ìœ„ë°˜", f"{summary['ê¸°ë³¸ ê·œì¹™ ìœ„ë°˜ ìˆ˜']:,}ê±´")
    with col3:
        st.metric("GPT ê²€ì¦ ìˆ˜í–‰", f"{summary['GPT ê²€ì¦ ìˆ˜í–‰ ìˆ˜']:,}ê±´")
    with col4:
        st.metric("GPT ê²€ì¦ ìœ„ë°˜", f"{summary['GPT ê²€ì¦ ìœ„ë°˜ ìˆ˜']:,}ê±´")
    
    # 2. ë°ì´í„° í•„í„°ë§ ì˜µì…˜
    st.subheader("ğŸ” ìƒì„¸ ê²°ê³¼")
    filter_option = st.radio(
        "í‘œì‹œ í•­ëª©:",
        ["ì „ì²´ í•­ëª©", "ê·œì¹™ ìœ„ë°˜ í•­ëª©ë§Œ", "GPT ê²€ì¦ í•­ëª©ë§Œ"],
        horizontal=True
    )
    
    # 3. í•„í„°ë§ëœ ê²°ê³¼ í‘œì‹œ
    filtered_df = df.copy()
    if filter_option == "ê·œì¹™ ìœ„ë°˜ í•­ëª©ë§Œ":
        mask = results['ê¸°ë³¸ê·œì¹™ìœ„ë°˜'] | (results['GPTê²€ì¦ê²°ê³¼'] == True)
        filtered_df = df[mask]
    elif filter_option == "GPT ê²€ì¦ í•­ëª©ë§Œ":
        filtered_df = df[results['GPTê²€ì¦ìˆ˜í–‰']]
    
    # ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
    filtered_df['ìœ„ë°˜ì—¬ë¶€'] = results['ê¸°ë³¸ê·œì¹™ìœ„ë°˜']
    filtered_df['GPTê²€ì¦'] = results['GPTê²€ì¦ìˆ˜í–‰']
    filtered_df['ê²€ì¦ê²°ê³¼'] = results['GPTê²€ì¦ì„¤ëª…']
    
    # ìŠ¤íƒ€ì¼ ì ìš©
    st.dataframe(
        filtered_df.style.apply(lambda x: ['background-color: #fee2e2' if x['ìœ„ë°˜ì—¬ë¶€'] else '' for _ in x], axis=1),
        height=400
    )
    
    # 4. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    csv = filtered_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ê²€ì¦ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name=f'ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ_ê²€ì¦ê²°ê³¼_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
        mime='text/csv',
    )

def filter_by_included_prefixes(df):
    """íŠ¹ì • ì ‘ë‘ì–´ë¥¼ ê°€ì§„ ë¬¸ì„œë²ˆí˜¸ë§Œ ë¶„ì„ì— í¬í•¨"""
    if 'ë¬¸ì„œë²ˆí˜¸' not in df.columns:
        return df, 0
    
    # í¬í•¨í•  ì ‘ë‘ì–´ ëª©ë¡
    included_prefixes = [
        'GEX', 'POD', 'MEX', 'TED', 'TEF', 'RRA', 'RSA'
    ]
    
    # ë¬¸ì„œë²ˆí˜¸ ì ‘ë‘ì–´ í™•ì¸ì„ ìœ„í•œ í•¨ìˆ˜
    def has_included_prefix(doc_num):
        if pd.isna(doc_num):
            return False
        doc_str = str(doc_num).upper()
        for prefix in included_prefixes:
            if doc_str.startswith(prefix):
                return True
        return False
    
    # í¬í•¨í•  í•­ëª© í•„í„°ë§
    mask = df['ë¬¸ì„œë²ˆí˜¸'].apply(has_included_prefix)
    filtered_df = df[mask]
    
    # ì œì™¸ëœ í•­ëª© ìˆ˜ ë°˜í™˜
    excluded_count = len(df) - len(filtered_df)
    
    return filtered_df, excluded_count

def validate_basic_rules(df):
    """ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ 1ì°¨ ê²€ì¦ - ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ê³ ì†í™”"""
    violations = {}
    
    # ëª¨ë“  ê²€ì¦ì„ í•œ ë²ˆì— ì²˜ë¦¬
    if 'ê²°ì˜ê¸ˆì•¡' in df.columns and 'ë¶€ê°€ê°€ì¹˜ì„¸' in df.columns:
        violations['ë¶€ê°€ì„¸ì´ˆê³¼'] = (df['ë¶€ê°€ê°€ì¹˜ì„¸'] > df['ê²°ì˜ê¸ˆì•¡'] * 0.1)
    if 'ì¦ë¹™ìœ í˜•' in df.columns:
        violations['ì¦ë¹™ëˆ„ë½'] = df['ì¦ë¹™ìœ í˜•'].isna()
    if 'ë¯¸ì§€ê¸‰ê¸ˆ' in df.columns:
        violations['ë¯¸ì§€ê¸‰ì”ì•¡'] = (df['ë¯¸ì§€ê¸‰ê¸ˆ'] > 0)
    if 'ê³¼ì„¸ì‚¬ì—…ì—¬ë¶€' in df.columns and 'ì†Œë“ìœ í˜•' in df.columns:
        violations['ê³¼ì„¸ëˆ„ë½'] = (df['ê³¼ì„¸ì‚¬ì—…ì—¬ë¶€'] == 'Y') & (df['ì†Œë“ìœ í˜•'].isna())
    if 'ë°˜ë‚©ì—¬ë¶€' in df.columns:
        violations['ë°˜ë‚©ì²˜ë¦¬'] = (df['ë°˜ë‚©ì—¬ë¶€'] == 'Y')
    
    # ê²°ê³¼ë¥¼ í•œ ë²ˆì— DataFrameìœ¼ë¡œ ë³€í™˜
    return pd.DataFrame(violations, index=df.index)

def select_validation_targets(df, basic_violations, max_samples=100):
    """GPT ê²€ì¦ ëŒ€ìƒ ì„ ì • - ìµœëŒ€ 100ê°œë¡œ ì œí•œ"""
    # 1. ê¸°ë³¸ ê·œì¹™ ìœ„ë°˜ ì‹¬ê°ë„ ê³„ì‚°
    violation_count = basic_violations.sum(axis=1)
    severe_violations = violation_count[violation_count >= 2]  # 2ê°œ ì´ìƒ ê·œì¹™ ìœ„ë°˜
    
    # 2. ê³ ì•¡ ê±°ë˜ (ìƒìœ„ 0.5%)
    if 'ê²°ì˜ê¸ˆì•¡' in df.columns:
        high_amount = df.nlargest(min(int(len(df) * 0.005), 50), 'ê²°ì˜ê¸ˆì•¡').index
    else:
        high_amount = pd.Index([])
    
    # 3. ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëŒ€ìƒ ì„ ì •
    priority_items = pd.Index(list(set(severe_violations.index) | set(high_amount)))
    
    # 4. ì¶”ê°€ ìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
    remaining_count = max_samples - len(priority_items)
    if remaining_count > 0 and len(df) > len(priority_items):
        # ì•„ì§ ì„ íƒë˜ì§€ ì•Šì€ í•­ëª©ì—ì„œ ëœë¤ ìƒ˜í”Œë§
        remaining_items = df.index.difference(priority_items)
        random_samples = np.random.choice(remaining_items, 
                                        size=min(remaining_count, len(remaining_items)), 
                                        replace=False)
        final_targets = priority_items.union(random_samples)
    else:
        final_targets = priority_items
    
    return df.loc[final_targets]

def process_large_dataset(df, qa):
    """ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ - ìµœì í™” ë²„ì „"""
    # 1. ê¸°ë³¸ ê·œì¹™ ê²€ì¦ (ë²¡í„°í™” ì—°ì‚°)
    basic_violations = validate_basic_rules(df)
    
    # 2. GPT ê²€ì¦ ëŒ€ìƒ ì„ ì • (ìµœëŒ€ 100ê°œ)
    target_items = select_validation_targets(df, basic_violations)
    total_targets = len(target_items)
    
    st.write(f"ì „ì²´ {len(df):,}ê°œ ì¤‘ {total_targets}ê°œ í•­ëª©ì— ëŒ€í•´ ìƒì„¸ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    # 3. ì„ ì •ëœ í•­ëª©ë“¤ì— ëŒ€í•´ GPT ê²€ì¦ ìˆ˜í–‰
    gpt_results = []
    
    with st.spinner(f'GPT ê²€ì¦ ì§„í–‰ ì¤‘... (0/{total_targets})'):
        progress_bar = st.progress(0)
        progress_text = st.empty()
        
        for i, (_, row) in enumerate(target_items.iterrows(), 1):
            # GPT ê²€ì¦
            result = validate_item(row.to_dict(), qa)
            gpt_results.append(result)
            
            # ì§„í–‰ìƒíƒœ ì—…ë°ì´íŠ¸ (10ê°œ ë‹¨ìœ„ë¡œ)
            if i % 10 == 0 or i == total_targets:
                progress = i / total_targets
                progress_bar.progress(progress)
                progress_text.text(f'GPT ê²€ì¦ ì§„í–‰ ì¤‘... ({i}/{total_targets})')
    
    # 4. ê²°ê³¼ ì¢…í•© (ë²¡í„°í™” ì—°ì‚°)
    final_results = pd.DataFrame(index=df.index)
    final_results['ê¸°ë³¸ê·œì¹™ìœ„ë°˜'] = basic_violations.any(axis=1)
    final_results['ìœ„ë°˜ê·œì¹™ìˆ˜'] = basic_violations.sum(axis=1)
    
    # GPT ê²€ì¦ ê²°ê³¼ ë§¤í•‘
    final_results['GPTê²€ì¦ìˆ˜í–‰'] = final_results.index.isin(target_items.index)
    
    # GPT ê²€ì¦ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    gpt_df = pd.DataFrame(gpt_results, index=target_items.index)
    final_results.loc[target_items.index, 'GPTê²€ì¦ê²°ê³¼'] = gpt_df['violation']
    final_results.loc[target_items.index, 'GPTê²€ì¦ì„¤ëª…'] = gpt_df['explanation']
    
    # 5. ìœ„ë°˜ í•­ëª© ìš”ì•½
    violation_summary = {
        'ì „ì²´ ë°ì´í„° ìˆ˜': len(df),
        'ê¸°ë³¸ ê·œì¹™ ìœ„ë°˜ ìˆ˜': basic_violations.any(axis=1).sum(),
        'GPT ê²€ì¦ ìˆ˜í–‰ ìˆ˜': len(target_items),
        'GPT ê²€ì¦ ìœ„ë°˜ ìˆ˜': gpt_df['violation'].sum() if not gpt_df.empty else 0
    }
    
    return final_results, violation_summary

# ë©”ì¸ ì•± ì‹¤í–‰ íë¦„ ìˆ˜ì •
def main():
    # ì‚¬ì´ë“œë°”ì— ì•± ì„¤ëª…
    with st.sidebar:
        st.markdown("## ğŸ” ì•± ì‚¬ìš©ë²•")
        st.markdown("""
        1. `backdata` í´ë”ì— ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
        2. ì•±ì´ ìë™ìœ¼ë¡œ í´ë” ë‚´ íŒŒì¼ì„ ê°ì§€í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.
        3. íŒŒì¼ì„ ì„ íƒí•˜ê³  'ê²€ì¦ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
        4. ê° í•­ëª©ì´ KAIST íšŒê³„ ê·œì •ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ìë™ ê²€ì¦í•©ë‹ˆë‹¤.
        5. ê²€ì¦ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  í•„ìš”ì‹œ CSVë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
        """)
        
        st.markdown("## âš ï¸ ì£¼ì˜ì‚¬í•­")
        st.markdown("""
        - ì´ ë„êµ¬ëŠ” ê·œì • ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” í•­ëª©ì„ ì‹ë³„í•˜ëŠ” ê²ƒì´ë©°, ìµœì¢… íŒë‹¨ì€ íšŒê³„ ë‹´ë‹¹ìê°€ í•´ì•¼ í•©ë‹ˆë‹¤.
        - ê·œì • í•´ì„ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì™„ì „í•œ ìë™í™”ê°€ ì•„ë‹Œ ê²€í†  ë³´ì¡° ë„êµ¬ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        """)
    
    # ë²¡í„°DB ë¡œë“œ
    qa, retriever = load_vector_db()
    if qa is None or retriever is None:
        st.error("ë²¡í„°DB ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. 'chroma_db' í´ë”ê°€ ì¡´ì¬í•˜ê³  ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë°ì´í„° ë¡œë“œ (ìë™ ë§¤í•‘ í¬í•¨)
    df, auto_mapping = load_unpaid_data()
    if df is None:
        return
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(df.head(), height=200)
    
    # ì—´ ë§¤í•‘ ì„¹ì…˜
    st.subheader("ë°ì´í„° ì—´ ë§¤í•‘")
    
    # ìë™ ë§¤í•‘ ê²°ê³¼ í‘œì‹œ
    st.markdown("### ğŸ¤– ìë™ ë§¤í•‘ ê²°ê³¼")
    mapping_status = pd.DataFrame({
        'í•„ìˆ˜ í•„ë“œ': auto_mapping.keys(),
        'ë§¤í•‘ëœ ì—´': [auto_mapping[k] if auto_mapping[k] else "ë§¤í•‘ ì‹¤íŒ¨" for k in auto_mapping.keys()],
        'ìƒíƒœ': ['âœ… ì„±ê³µ' if auto_mapping[k] else 'âŒ ì‹¤íŒ¨' for k in auto_mapping.keys()]
    })
    st.dataframe(mapping_status, height=250)
    
    # ë§¤í•‘ ì‹¤íŒ¨í•œ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    failed_mappings = [field for field, value in auto_mapping.items() if not value]
    
    if failed_mappings:
        st.warning(f"ë‹¤ìŒ í•„ë“œì˜ ìë™ ë§¤í•‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {', '.join(failed_mappings)}")
        st.info("ìˆ˜ë™ìœ¼ë¡œ ë§¤í•‘ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.")
        
        # ìˆ˜ë™ ë§¤í•‘ UI
        columns = df.columns.tolist()
        col1, col2 = st.columns(2)
        
        with col1:
            for i, field in enumerate(failed_mappings[:len(failed_mappings)//2 + len(failed_mappings)%2]):
                auto_mapping[field] = st.selectbox(
                    f"{field} ì—´ ì„ íƒ",
                    options=["ì„ íƒí•˜ì§€ ì•ŠìŒ"] + columns,
                    key=f"manual_mapping_{field}"
                )
        
        with col2:
            for i, field in enumerate(failed_mappings[len(failed_mappings)//2 + len(failed_mappings)%2:]):
                auto_mapping[field] = st.selectbox(
                    f"{field} ì—´ ì„ íƒ",
                    options=["ì„ íƒí•˜ì§€ ì•ŠìŒ"] + columns,
                    key=f"manual_mapping_{field}"
                )
    
    # ê²€ì¦ ì‹œì‘ ë²„íŠ¼
    if st.button("ê²€ì¦ ì‹œì‘", use_container_width=True):
        # í•„ìˆ˜ í•„ë“œê°€ ëª¨ë‘ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
        unmapped_fields = [field for field, value in auto_mapping.items() if not value or value == "ì„ íƒí•˜ì§€ ì•ŠìŒ"]
        if unmapped_fields:
            st.error(f"ë‹¤ìŒ í•„ìˆ˜ í•„ë“œê°€ ë§¤í•‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(unmapped_fields)}")
            return
        
        # ë§¤í•‘ëœ ì—´ë§Œ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ ìƒì„±
        mapped_df = pd.DataFrame()
        for field, column in auto_mapping.items():
            mapped_df[field] = df[column]
        
        # 1. íŠ¹ì • ì ‘ë‘ì–´ ë¬¸ì„œë²ˆí˜¸ í•„í„°ë§ - ìˆ˜ì •ëœ ë¶€ë¶„
        with st.spinner("ë°ì´í„° í•„í„°ë§ ì¤‘..."):
            filtered_df, excluded_count = filter_by_included_prefixes(mapped_df)
            st.info(f"ë¬¸ì„œë²ˆí˜¸ í•„í„°ë§: ì „ì²´ {len(mapped_df):,}ê°œ ì¤‘ {len(filtered_df):,}ê°œê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤. (ì œì™¸: {excluded_count:,}ê°œ)")
            
            if len(filtered_df) == 0:
                st.warning("í•„í„°ë§ í›„ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
        
        # 2. ê¸°ë³¸ ê·œì¹™ ê²€ì¦ (ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬)
        with st.spinner("ê¸°ë³¸ ê·œì¹™ ê²€ì¦ ì¤‘..."):
            violations = validate_basic_rules(filtered_df)
            filtered_df['ìœ„ë°˜ì—¬ë¶€'] = violations.any(axis=1)
            filtered_df['ìœ„ë°˜ê·œì¹™ìˆ˜'] = violations.sum(axis=1)
            
            # ìœ„ë°˜ í•­ëª© ê°œìˆ˜
            violation_count = filtered_df['ìœ„ë°˜ì—¬ë¶€'].sum()
            total_count = len(filtered_df)
            
            # ë©”íŠ¸ë¦­ í‘œì‹œ
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì „ì²´ í•­ëª© ìˆ˜", f"{total_count:,}ê°œ")
            with col2:
                st.metric("ê·œì¹™ ìœ„ë°˜ í•­ëª© ìˆ˜", f"{violation_count:,}ê°œ")
            with col3:
                violation_pct = (violation_count / total_count * 100) if total_count > 0 else 0
                st.metric("ê·œì¹™ ìœ„ë°˜ ë¹„ìœ¨", f"{violation_pct:.1f}%")
        
        # 3. í•„í„°ë§ ì˜µì…˜
        filter_option = st.radio(
            "í‘œì‹œ í•­ëª©:",
            ["ì „ì²´ í•­ëª©", "ê·œì¹™ ìœ„ë°˜ í•­ëª©ë§Œ"],
            horizontal=True
        )
        
        # 4. í•„í„°ë§ëœ ê²°ê³¼ í‘œì‹œ
        display_df = filtered_df if filter_option == "ì „ì²´ í•­ëª©" else filtered_df[filtered_df['ìœ„ë°˜ì—¬ë¶€']]
        
        # 5. ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
        st.dataframe(
            display_df.style.apply(
                lambda x: ['background-color: #fee2e2' if x['ìœ„ë°˜ì—¬ë¶€'] else '' for _ in x], 
                axis=1
            ),
            height=400
        )
        
        # 6. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        csv = display_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ê²€ì¦ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=f'ë¯¸ì§€ê¸‰ê¸ˆëª…ì„¸ì„œ_ê²€ì¦ê²°ê³¼_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mime='text/csv',
        )
        
        # 7. GPT ê²€ì¦ ì˜µì…˜ (ë³„ë„ ë²„íŠ¼ìœ¼ë¡œ)
        st.subheader("ğŸ¤– GPT ê·œì • ê²€ì¦ (ì„ íƒì‚¬í•­)")
        st.warning("GPT ê²€ì¦ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # GPT ê²€ì¦ ë²„íŠ¼
        col1, col2 = st.columns([1, 2])
        with col1:
            sample_size = st.number_input(
                "ê²€ì¦í•  í•­ëª© ìˆ˜", 
                min_value=1, 
                max_value=min(30, violation_count), 
                value=min(10, violation_count)
            )
        
        with col2:
            if st.button("ì„ íƒí•œ ê°œìˆ˜ë§Œí¼ GPTë¡œ ìƒì„¸ ê²€ì¦", use_container_width=True):
                # ìœ„ë°˜ í•­ëª© ì¤‘ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
                if violation_count > 0:
                    gpt_targets = filtered_df[filtered_df['ìœ„ë°˜ì—¬ë¶€']].sample(n=min(sample_size, violation_count))
                    
                    # GPT ê²€ì¦ ìˆ˜í–‰
                    with st.spinner(f'GPT ê²€ì¦ ì§„í–‰ ì¤‘... (0/{len(gpt_targets)})'):
                        progress_bar = st.progress(0)
                        progress_text = st.empty()
                        
                        gpt_results = []
                        for i, (_, row) in enumerate(gpt_targets.iterrows(), 1):
                            # GPT ê²€ì¦
                            result = validate_item(row.to_dict(), qa)
                            gpt_results.append(result)
                            
                            # ì§„í–‰ìƒíƒœ ì—…ë°ì´íŠ¸
                            progress = i / len(gpt_targets)
                            progress_bar.progress(progress)
                            progress_text.text(f'GPT ê²€ì¦ ì§„í–‰ ì¤‘... ({i}/{len(gpt_targets)})')
                        
                        # GPT ê²€ì¦ ê²°ê³¼ í‘œì‹œ
                        st.subheader("GPT ê²€ì¦ ê²°ê³¼")
                        for i, (_, row) in enumerate(gpt_targets.iterrows()):
                            with st.expander(f"í•­ëª© {i+1}: {row.get('ì§€ì¶œë‚´ì—­', '')[:50]}"):
                                st.write("**í•­ëª© ì •ë³´:**")
                                for key, val in row.items():
                                    if not pd.isna(val):
                                        st.write(f"- {key}: {val}")
                                
                                st.write("**ê²€ì¦ ê²°ê³¼:**")
                                if gpt_results[i]["violation"]:
                                    st.error(gpt_results[i]["explanation"])
                                else:
                                    st.success("ê·œì • ì¤€ìˆ˜: " + gpt_results[i]["explanation"])
                                
                                if gpt_results[i]["regulation_reference"] != "N/A":
                                    st.info(f"**ê´€ë ¨ ê·œì •:** {gpt_results[i]['regulation_reference']}")
                else:
                    st.info("ê·œì¹™ ìœ„ë°˜ í•­ëª©ì´ ì—†ì–´ GPT ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 